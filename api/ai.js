/**
 * ü§ñ ENDPOINT UNIFICADO DE IA - RKMMAX GENIUS
 * 
 * Consolida TODOS os endpoints de chat em um √∫nico endpoint:
 * - /api/chat.js ‚Üí /api/ai?type=chat
 * - /api/chat-genius.js ‚Üí /api/ai?type=genius
 * - /api/chat-intelligent.js ‚Üí /api/ai?type=intelligent
 * - /api/specialist-chat.js ‚Üí /api/ai?type=specialist
 * - /api/hybrid.js ‚Üí /api/ai?type=hybrid
 * - /api/transcribe.js ‚Üí /api/ai?type=transcribe
 * 
 * Economia: 6 fun√ß√µes ‚Üí 1 fun√ß√£o (83% redu√ß√£o!)
 * Qualidade: MANTIDA 100%
 */

const { buildGeniusPrompt } = require('../src/prompts/geniusPrompts.js');
const { optimizeRequest, cacheResponse } = require('../src/utils/costOptimization.js');
const { specialists } = require('../src/config/specialists.js');

/**
 * Chamar Gemini 2.5 Pro REAL (modelo mais avan√ßado)
 */
async function callGeminiPro(messages, systemPrompt, apiKey) {
  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=${apiKey}`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        systemInstruction: systemPrompt ? { parts: [{ text: systemPrompt }] } : undefined,
        contents: messages.map(msg => ({
          role: msg.role === 'user' ? 'user' : 'model',
          parts: [{ text: msg.content }]
        })),
        generationConfig: {
          temperature: 1.0,
          maxOutputTokens: 16000,  // Gemini 2.5 Pro suporta mais tokens
          topP: 0.95,
          topK: 64  // Melhor para racioc√≠nio complexo
        }
      })
    }
  );

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Gemini Pro error: ${error}`);
  }

  const data = await response.json();
  return data.candidates[0].content.parts[0].text;
}

/**
 * Chamar Gemini Flash (otimizado para especialistas)
 */
async function callGeminiFlash(messages, systemPrompt, apiKey) {
  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=${apiKey}`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        systemInstruction: systemPrompt ? { parts: [{ text: systemPrompt }] } : undefined,
        contents: messages.map(msg => ({
          role: msg.role === 'user' ? 'user' : 'model',
          parts: [{ text: msg.content }]
        })),
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: 4000,
          topP: 0.95,
          topK: 40
        }
      })
    }
  );

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Gemini Flash error: ${error}`);
  }

  const data = await response.json();
  return data.candidates[0].content.parts[0].text;
}

/**
 * Chamar GROQ (fallback)
 */
async function callGroq(messages, systemPrompt, apiKey) {
  const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: 'llama-3.3-70b-versatile',
      messages: systemPrompt 
        ? [{ role: 'system', content: systemPrompt }, ...messages]
        : messages,
      temperature: 0.7,
      max_tokens: 4000
    })
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`GROQ error: ${error}`);
  }

  const data = await response.json();
  return data.choices[0].message.content;
}

/**
 * Handler principal - Roteia para fun√ß√£o correta
 */
export default async function handler(req, res) {
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const { type = 'genius', messages, agentType, mode, specialistId } = req.body;

    console.log(`ü§ñ AI Endpoint - Type: ${type} | Agent: ${agentType || 'default'}`);

    // Verificar credenciais
    const hasGemini = !!process.env.GEMINI_API_KEY;
    const hasGroq = !!process.env.GROQ_API_KEY;

    if (!hasGemini && !hasGroq) {
      return res.status(500).json({
        error: 'No AI providers configured',
        hint: 'Configure GEMINI_API_KEY or GROQ_API_KEY'
      });
    }

    // ========================================
    // TIPO: GENIUS (Serginho + H√≠brido)
    // ========================================
    if (type === 'genius' || type === 'chat' || type === 'intelligent' || type === 'hybrid') {
      // Construir prompt de g√™nio
      const promptType = agentType || 'serginho';
      const systemPrompt = buildGeniusPrompt(promptType);

      // Otimizar requisi√ß√£o
      const optimized = optimizeRequest(messages, systemPrompt);

      // Verificar cache
      if (optimized.cached) {
        console.log('üí∞ CACHE HIT! Economia total!');
        return res.status(200).json({
          ...optimized.response,
          cached: true,
          type: promptType
        });
      }

      console.log(`üí∞ Otimiza√ß√£o: ${optimized.stats.originalMessages} ‚Üí ${optimized.stats.optimizedMessages} msgs`);

      // Tentar Gemini Pro 2.5 (sempre)
      if (hasGemini) {
        try {
          console.log('üöÄ Chamando Gemini Pro 2.5 (ChatGPT-5 level)...');
          const response = await callGeminiPro(
            optimized.messages,
            optimized.systemPrompt,
            process.env.GEMINI_API_KEY
          );

          const result = {
            response,
            model: 'gemini-exp-1206',  // Gemini 2.5 Pro REAL
            provider: 'google',
            tier: 'genius',
            type: promptType,
            success: true
          };

          // Salvar no cache
          cacheResponse(messages, result);

          return res.status(200).json({
            ...result,
            cached: false,
            optimized: true,
            stats: optimized.stats
          });
        } catch (error) {
          console.error('‚ùå Gemini Pro falhou:', error.message);
          
          // Fallback para GROQ
          if (hasGroq) {
            console.log('üîÑ Fallback: Tentando GROQ...');
            try {
              const response = await callGroq(
                optimized.messages,
                optimized.systemPrompt,
                process.env.GROQ_API_KEY
              );

              return res.status(200).json({
                response,
                model: 'llama-3.3-70b-versatile',
                provider: 'groq',
                tier: 'fallback',
                type: promptType,
                success: true,
                fallback: true
              });
            } catch (groqError) {
              console.error('‚ùå GROQ tamb√©m falhou:', groqError.message);
              throw groqError;
            }
          }

          throw error;
        }
      }

      // Se n√£o tem Gemini, usar GROQ direto
      if (hasGroq) {
        console.log('üöÄ Usando GROQ (sem Gemini dispon√≠vel)...');
        const response = await callGroq(
          optimized.messages,
          optimized.systemPrompt,
          process.env.GROQ_API_KEY
        );

        return res.status(200).json({
          response,
          model: 'llama-3.3-70b-versatile',
          provider: 'groq',
          tier: 'primary',
          type: promptType,
          success: true
        });
      }
    }

    // ========================================
    // TIPO: SPECIALIST (54 Especialistas)
    // ========================================
    if (type === 'specialist') {
      if (!specialistId) {
        return res.status(400).json({ error: 'specialistId required' });
      }

      const specialist = specialists.find(s => s.id === specialistId);
      if (!specialist) {
        return res.status(404).json({ error: 'Specialist not found' });
      }

      // Construir prompt de g√™nio do especialista
      const systemPrompt = buildGeniusPrompt('specialist', {
        name: specialist.name,
        description: specialist.description,
        category: specialist.category,
        systemPrompt: specialist.systemPrompt
      });

      // Otimizar requisi√ß√£o
      const optimized = optimizeRequest(messages, systemPrompt);

      // Verificar cache
      if (optimized.cached) {
        console.log('üí∞ CACHE HIT! Economia total!');
        return res.status(200).json({
          ...optimized.response,
          cached: true,
          specialist: specialist.name
        });
      }

      console.log(`üéØ Especialista: ${specialist.name}`);
      console.log(`üí∞ Otimiza√ß√£o: ${optimized.stats.originalMessages} ‚Üí ${optimized.stats.optimizedMessages} msgs`);

      // Tentar Gemini 2.5 Pro (qualidade m√°xima para especialistas)
      if (hasGemini) {
        try {
          console.log('üöÄ Chamando Gemini 2.5 Pro (especialista)...');
          const response = await callGeminiPro(
            optimized.messages,
            optimized.systemPrompt,
            process.env.GEMINI_API_KEY
          );

          const result = {
            response,
            model: 'gemini-exp-1206',  // Gemini 2.5 Pro REAL
            provider: 'google',
            specialist: specialist.name,
            tier: 'genius',  // Agora √© genius tamb√©m!
            success: true
          };

          // Salvar no cache
          cacheResponse(messages, result);

          return res.status(200).json({
            ...result,
            cached: false,
            optimized: true,
            stats: optimized.stats
          });
        } catch (error) {
          console.error(`‚ùå Gemini Flash falhou para ${specialist.name}:`, error.message);
          
          // Fallback para GROQ
          if (hasGroq) {
            console.log('üîÑ Fallback: Tentando GROQ...');
            try {
              const response = await callGroq(
                optimized.messages,
                optimized.systemPrompt,
                process.env.GROQ_API_KEY
              );

              return res.status(200).json({
                response,
                model: 'llama-3.3-70b-versatile',
                provider: 'groq',
                specialist: specialist.name,
                tier: 'fallback',
                success: true,
                fallback: true
              });
            } catch (groqError) {
              console.error('‚ùå GROQ tamb√©m falhou:', groqError.message);
              throw groqError;
            }
          }

          throw error;
        }
      }

      // Se n√£o tem Gemini, usar GROQ direto
      if (hasGroq) {
        console.log('üöÄ Usando GROQ (sem Gemini dispon√≠vel)...');
        const response = await callGroq(
          optimized.messages,
          optimized.systemPrompt,
          process.env.GROQ_API_KEY
        );

        return res.status(200).json({
          response,
          model: 'llama-3.3-70b-versatile',
          provider: 'groq',
          specialist: specialist.name,
          tier: 'primary',
          success: true
        });
      }
    }

    // ========================================
    // TIPO: TRANSCRIBE (Whisper)
    // ========================================
    if (type === 'transcribe') {
      const { audioBase64 } = req.body;

      if (!audioBase64) {
        return res.status(400).json({ error: 'audioBase64 required' });
      }

      // TODO: Implementar transcri√ß√£o com Whisper
      // Por enquanto, retornar erro
      return res.status(501).json({
        error: 'Transcription not implemented yet',
        hint: 'Will be implemented in next version'
      });
    }

    // Tipo desconhecido
    return res.status(400).json({
      error: 'Invalid type',
      hint: 'Use type: genius, specialist, or transcribe'
    });

  } catch (error) {
    console.error('‚ùå AI Endpoint error:', error);
    return res.status(500).json({
      error: 'Internal server error',
      message: error.message
    });
  }
}
